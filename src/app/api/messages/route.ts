import { NextRequest, NextResponse } from "next/server";
import { prisma } from "@/lib/prisma";

type ChatMessage = { from: "user" | "bot"; text: string };

export async function POST(req: NextRequest) {
  const { messages }: { messages: ChatMessage[] } = await req.json();

  const systemPrompt = {
    role: "system",
    content: `
  Voc√™ √© um atendente virtual de uma pizzaria. Responda sempre em portugu√™s do Brasil.
  
  Siga estas regras ao conversar:
  - Ofere√ßa apenas pizzas (Margherita, Calabresa, Portuguesa, Quatro Queijos, Frango com Catupiry), bebidas (Refrigerante Lata, 600ml ou 2 Litros ‚Äî Coca, Guaran√°, etc.) e sobremesas (Pudim, Mousse, Sorvete).
  - Nunca ofere√ßa itens fora dessa lista. N√£o fale sobre descontos ou promo√ß√µes.
  - Seja direto, educado e simp√°tico. Frases curtas, sem exageros.
  - Se o cliente pedir pizza, ofere√ßa bebida. Se pedir bebida, ofere√ßa sobremesa.
  - Caso recuse, sugira outro item do mesmo grupo.
  - Nunca saia do contexto de pizzaria.
  
  ‚ö†Ô∏è **Fluxo obrigat√≥rio:**
  1. Primeiro, colete a escolha da pizza.
  2. Depois, pergunte sobre bebida.
  3. Depois, pergunte sobre sobremesa.
  4. Somente **ap√≥s o cliente responder sobre a sobremesa** (ou recusar), pergunte o CEP para o endere√ßo.
  
  Ao receber o CEP:
  - Busque a rua correspondente automaticamente (ex: ‚ÄúCEP 74620-385 corresponde √† Rua 3. Est√° correto?‚Äù).
  - Pe√ßa os dados restantes: n√∫mero, quadra, lote, complemento.
  
  üõë Nunca mencione valores.
  ‚úÖ Seu foco √© **coletar o pedido completo e o endere√ßo** com clareza.
  `,
  };

  const openAiMessages = [
    systemPrompt,
    ...messages.map((m: ChatMessage) => ({
      role: m.from === "user" ? "user" : "assistant",
      content: m.text,
    })),
  ];

  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    return NextResponse.json(
      { reply: "Chave da OpenAI n√£o configurada." },
      { status: 500 }
    );
  }

  const body = {
    model: "gpt-3.5-turbo",
    messages: openAiMessages,
    temperature: 0.3,
    max_tokens: 300,
  };

  const lastUser = messages[messages.length - 1];
  await prisma.message.create({
    data: {
      from: lastUser.from,
      text: lastUser.text,
    },
  });

  const res = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify(body),
  });

  if (!res.ok) {
    return NextResponse.json(
      { reply: "Erro ao acessar a OpenAI." },
      { status: 500 }
    );
  }

  const data = await res.json();
  const reply = data.choices?.[0]?.message?.content ?? "Desculpe, n√£o entendi.";

  await prisma.message.create({
    data: {
      from: "bot",
      text: reply,
    },
  });

  return NextResponse.json({ reply });
}

export async function GET() {
  try {
    const messages = await prisma.message.findMany({
      orderBy: { createdAt: "asc" },
      select: { id: true, from: true, text: true, createdAt: true },
    });
    return NextResponse.json(messages, { status: 200 });
  } catch {
    return NextResponse.json(
      { error: "Erro ao buscar hist√≥rico de mensagens." },
      { status: 500 }
    );
  }
}
